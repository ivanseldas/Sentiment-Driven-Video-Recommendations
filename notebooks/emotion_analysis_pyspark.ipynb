{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GtdXlqi2_T6N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBslCLdG_T6Q",
        "outputId": "84eb4e80-512b-4587-b449-615b10d54edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 184673 entries, 0 to 184672\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count   Dtype              \n",
            "---  ------             --------------   -----              \n",
            " 0   comment_id         184673 non-null  object             \n",
            " 1   author             184673 non-null  object             \n",
            " 2   author_channel_id  184673 non-null  object             \n",
            " 3   text               184673 non-null  object             \n",
            " 4   like_count         184673 non-null  float64            \n",
            " 5   published_at       184673 non-null  datetime64[ns, UTC]\n",
            " 6   updated_at         184673 non-null  datetime64[ns, UTC]\n",
            " 7   totalReplyCount    184673 non-null  float64            \n",
            " 8   video_id           184673 non-null  object             \n",
            " 9   translation        184670 non-null  object             \n",
            " 10  clean_text         184670 non-null  object             \n",
            " 11  sentiment          184670 non-null  float64            \n",
            "dtypes: datetime64[ns, UTC](2), float64(3), object(7)\n",
            "memory usage: 16.9+ MB\n"
          ]
        }
      ],
      "source": [
        "# Load the parquet dataframe in pandas\n",
        "pd_df = pd.read_parquet(\"../data/clean_data/df_trans_sent_comments.parquet\")\n",
        "\n",
        "# Display info of dataframe\n",
        "pd_df.info()\n",
        "\n",
        "# Drop Nan val# ues\n",
        "pd_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBz0P1r-_T6Q"
      },
      "source": [
        "# Sentiment Analysis using RoBERTa\n",
        "* NLP Transformer-based Models used for Sentiment Analysis\n",
        "\n",
        "* RoBERTa: Robustly Optimized BERT Approach\n",
        "* BERT: Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "* Optimised using datasets from Huggingface: structure optimised for NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ivanseldasp/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "983b73b92abe49529800f30ece1b4e40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/184670 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from datasets import Dataset\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis model with GPU support if available\n",
        "sentiment_model = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=5, device=0)\n",
        "\n",
        "# Define a function to process a batch of texts and return the raw output without truncating the tokens\n",
        "def apply_sentiment(batch):\n",
        "    # Ensure 'clean_text' is a list of texts in the batch\n",
        "    texts = batch['clean_text_truncated']\n",
        "    \n",
        "    # Process each text without truncating tokens\n",
        "    sentiments = [sentiment_model(text) for text in texts]  # No token truncation, raw model output\n",
        "    \n",
        "    # Add the raw sentiment analysis result back to the batch\n",
        "    batch['emotions'] = sentiments\n",
        "    \n",
        "    return batch\n",
        "\n",
        "# Safe copy of the dataframe\n",
        "df_test = pd_df.copy()\n",
        "\n",
        "# Truncate the text up to 512 tokens\n",
        "def truncate_text(text):\n",
        "    return text[:512]\n",
        "df_test[\"clean_text_truncated\"] = df_test[\"clean_text\"].apply(truncate_text)\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas and then to a Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Apply the sentiment analysis model in batches\n",
        "dataset = dataset.map(apply_sentiment, batched=True, batch_size=64)\n",
        "\n",
        "# Convert the dataset back to a pandas DataFrame if needed\n",
        "df_result = dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert RoBERTa output into a dictionary 'label:score'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the function\n",
        "def get_label_dict(emotion_list):\n",
        "    emotion_dict = {}\n",
        "    for label in emotion_list[0]:\n",
        "        emotion_dict[label['label']] = label['score']\n",
        "    return emotion_dict\n",
        "\n",
        "# Apply the function to the column\n",
        "df_result[\"emotions\"] = df_result[\"emotions\"].apply(get_label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save dataframe locally\n",
        "df_result.to_parquet(\"../data/clean_data/df_comments_emotions_nlp.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark for Sentiment Analysis: initial GPU configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Phy8iiAW5Ho7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.storagelevel import StorageLevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f5mWITN__T6P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "24/10/24 00:40:59 WARN Utils: Your hostname, DESKTOP-AEUBGUH resolves to a loopback address: 127.0.1.1; using 172.17.252.233 instead (on interface eth0)\n",
            "24/10/24 00:40:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/10/24 00:41:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "# Create a Spark sesion\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ggl7UVFD34rC"
      },
      "outputs": [],
      "source": [
        "# Amply the memory up to 8GB\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 8000 * 1024 * 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pISe2vhU1uah",
        "outputId": "6e9d628c-6c95-4e0a-c5f4-94dfd7554807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default size of broadcast table is 8000.0 MB.\n"
          ]
        }
      ],
      "source": [
        "# Get the threshold value and remove the non-numeric part (the 'b' character for bytes)\n",
        "threshold_value = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").rstrip('b')\n",
        "\n",
        "# Convert the cleaned value to an integer and calculate the size in MB\n",
        "size = int(threshold_value) / (1024 * 1024)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Default size of broadcast table is {size} MB.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load PySpark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/24 00:41:55 WARN TaskSetManager: Stage 0 contains a task of very large size (9365 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------------+--------------------+--------------------+----------+-------------------+-------------------+---------------+-----------+--------------------+--------------------+---------+--------------------+-----------------+--------------------+\n",
            "|          comment_id|            author|   author_channel_id|                text|like_count|       published_at|         updated_at|totalReplyCount|   video_id|         translation|          clean_text|sentiment|clean_text_truncated|__index_level_0__|            emotions|\n",
            "+--------------------+------------------+--------------------+--------------------+----------+-------------------+-------------------+---------------+-----------+--------------------+--------------------+---------+--------------------+-----------------+--------------------+\n",
            "|UgxqJ2eO2p7w2NEvB...|@emmanuelamama1991|UCdUooL3DTt3Wj0M3...|great video,helpful.|       1.0|2024-08-17 15:07:48|2024-08-17 15:07:48|            1.0|qtlUwwtvuEg|great video,helpful.|  great videohelpful|   0.6249|  great videohelpful|                0|{love -> NULL, op...|\n",
            "|UgzmXwaNzivNHqjfR...| @dr.alexshayo6972|UCN56M0YoTn18rd_K...|Thank you for suc...|       4.0|2023-05-01 16:48:21|2023-05-01 16:48:21|            1.0|qtlUwwtvuEg|Thank you for suc...|thank well explai...|   0.5574|thank well explai...|                1|{love -> NULL, op...|\n",
            "|UgyySzt4xOZ1hKdS9...|   @mohdkashif4596|UCr5Ua5AsmM6phaRu...|very useful tools...|       2.0|2023-07-20 13:33:23|2023-07-20 13:33:23|            1.0|qtlUwwtvuEg|very useful tools...|useful tool thank...|   0.8225|useful tool thank...|                2|{love -> NULL, op...|\n",
            "|UgyHVo-IOZw_-uLb-...|        @pipedrmmr|UC5KmDvJCDtoM31gj...|This is great inf...|      13.0|2023-06-25 02:36:50|2023-06-25 02:36:50|            1.0|qtlUwwtvuEg|This is great inf...|great information...|   0.9062|great information...|                3|{love -> NULL, op...|\n",
            "|UgyIT65ucXCc-4loB...|      @anamnaz2527|UCriCWejgmnsDkaX7...|first two are not...|       1.0|2023-05-31 18:41:21|2023-05-31 18:41:21|            1.0|qtlUwwtvuEg|first two are not...|first two goof ai...|    0.872|first two goof ai...|                4|{love -> NULL, op...|\n",
            "+--------------------+------------------+--------------------+--------------------+----------+-------------------+-------------------+---------------+-----------+--------------------+--------------------+---------+--------------------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load dataframe\n",
        "df_result = pd.read_parquet(\"../data/clean_data/df_comments_emotions_nlp.parquet\")\n",
        "\n",
        "# Create spark dataframe from pandas dataframe\n",
        "df_spark = spark.createDataFrame(df_result)\n",
        "df_spark.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('comment_id', 'string'),\n",
              " ('author', 'string'),\n",
              " ('author_channel_id', 'string'),\n",
              " ('text', 'string'),\n",
              " ('like_count', 'double'),\n",
              " ('published_at', 'timestamp'),\n",
              " ('updated_at', 'timestamp'),\n",
              " ('totalReplyCount', 'double'),\n",
              " ('video_id', 'string'),\n",
              " ('translation', 'string'),\n",
              " ('clean_text', 'string'),\n",
              " ('sentiment', 'double'),\n",
              " ('clean_text_truncated', 'string'),\n",
              " ('__index_level_0__', 'bigint'),\n",
              " ('emotions', 'map<string,double>')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_spark.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark: UDF function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Diccionario que mapea emociones a un valor entre -1 y 1\n",
        "emotion_sentiment_map = {\n",
        "    'admiration': 1,\n",
        "    'approval': 0.8,\n",
        "    'gratitude': 1,\n",
        "    'neutral': 0,\n",
        "    'optimism': 0.7,\n",
        "    'disapproval': -0.7,\n",
        "    'realization': 0.5,\n",
        "    'curiosity': 0.4,\n",
        "    'excitement': 0.9,\n",
        "    'annoyance': -0.6,\n",
        "    'joy': 1,\n",
        "    'disappointment': -0.6,\n",
        "    'confusion': -0.3,\n",
        "    'surprise': 0.2,\n",
        "    'love': 1,\n",
        "    'pride': 0.9,\n",
        "    'sadness': -0.8,\n",
        "    'caring': 0.8,\n",
        "    'amusement': 0.7,\n",
        "    'anger': -0.9,\n",
        "    'desire': 0.5,\n",
        "    'disgust': -1,\n",
        "    'relief': 0.6,\n",
        "    'fear': -0.9,\n",
        "    'remorse': -0.7,\n",
        "    'grief': -0.9,\n",
        "    'embarrassment': -0.4,\n",
        "    'nervousness': -0.5\n",
        "}\n",
        "\n",
        "# Check whether the input is empty\n",
        "def calculate_sentiment(emotions):\n",
        "    if len(emotions) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # return (float(len(emotions)))\n",
        "\n",
        "    # Create an initial list with all the emotion's scores\n",
        "    initial_score_list = []\n",
        "    for label in emotions:\n",
        "        if emotions[label] is not None:\n",
        "            score = emotions[label]\n",
        "            initial_score_list.append(score)\n",
        "\n",
        "    # Sum the total initial score\n",
        "    initial_score_total = sum(initial_score_list)\n",
        "\n",
        "    # Multiply each percentage of the emotion per the value from the emotion_dictionary_values\n",
        "    emotion_score_list = []\n",
        "    for label in emotions:\n",
        "        if emotions[label] is not None:\n",
        "            score = emotions[label] / initial_score_total\n",
        "            final_score = emotion_sentiment_map[label] * score\n",
        "            emotion_score_list.append(final_score)\n",
        "\n",
        "    # Sum the total value pondered\n",
        "    return sum(emotion_score_list)\n",
        "\n",
        "sentiment_model = udf(calculate_sentiment, FloatType())\n",
        "\n",
        "df_with_sentiment = df_spark.withColumn(\"emotion_score\", sentiment_model(df_spark[\"emotions\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/24 00:48:27 WARN TaskSetManager: Stage 1 contains a task of very large size (9365 KiB). The maximum recommended task size is 1000 KiB.\n",
            "[Stage 1:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+----------+-------------------+-------------------+---------------+-----------+--------------------+--------------------+---------+--------------------+-----------------+--------------------+-------------+\n",
            "|          comment_id|              author|   author_channel_id|                text|like_count|       published_at|         updated_at|totalReplyCount|   video_id|         translation|          clean_text|sentiment|clean_text_truncated|__index_level_0__|            emotions|emotion_score|\n",
            "+--------------------+--------------------+--------------------+--------------------+----------+-------------------+-------------------+---------------+-----------+--------------------+--------------------+---------+--------------------+-----------------+--------------------+-------------+\n",
            "|UgxqJ2eO2p7w2NEvB...|  @emmanuelamama1991|UCdUooL3DTt3Wj0M3...|great video,helpful.|       1.0|2024-08-17 15:07:48|2024-08-17 15:07:48|            1.0|qtlUwwtvuEg|great video,helpful.|  great videohelpful|   0.6249|  great videohelpful|                0|{love -> NULL, op...|    0.9666206|\n",
            "|UgzmXwaNzivNHqjfR...|   @dr.alexshayo6972|UCN56M0YoTn18rd_K...|Thank you for suc...|       4.0|2023-05-01 16:48:21|2023-05-01 16:48:21|            1.0|qtlUwwtvuEg|Thank you for suc...|thank well explai...|   0.5574|thank well explai...|                1|{love -> NULL, op...|   0.98379844|\n",
            "|UgyySzt4xOZ1hKdS9...|     @mohdkashif4596|UCr5Ua5AsmM6phaRu...|very useful tools...|       2.0|2023-07-20 13:33:23|2023-07-20 13:33:23|            1.0|qtlUwwtvuEg|very useful tools...|useful tool thank...|   0.8225|useful tool thank...|                2|{love -> NULL, op...|    0.9885464|\n",
            "|UgyHVo-IOZw_-uLb-...|          @pipedrmmr|UC5KmDvJCDtoM31gj...|This is great inf...|      13.0|2023-06-25 02:36:50|2023-06-25 02:36:50|            1.0|qtlUwwtvuEg|This is great inf...|great information...|   0.9062|great information...|                3|{love -> NULL, op...|    0.9950938|\n",
            "|UgyIT65ucXCc-4loB...|        @anamnaz2527|UCriCWejgmnsDkaX7...|first two are not...|       1.0|2023-05-31 18:41:21|2023-05-31 18:41:21|            1.0|qtlUwwtvuEg|first two are not...|first two goof ai...|    0.872|first two goof ai...|                4|{love -> NULL, op...|   0.98620284|\n",
            "|UgwTD-F3eXXyuu69D...|     @naginakhan2814|UCZl0m7mBgEYH2pDt...|         Informative|       2.0|2023-08-05 20:58:19|2023-08-05 20:58:19|            1.0|qtlUwwtvuEg|         Informative|         informative|      0.0|         informative|                5|{love -> NULL, op...|  0.014072355|\n",
            "|Ugwb-J02-9hQgzhml...|@enriquetabuenave...|UCLXTa307tK48oFZO...|I am 66 years old...|       3.0|2023-08-03 16:52:57|2023-08-03 16:52:57|            1.0|qtlUwwtvuEg|I am 66 years old...|year old elementa...|   0.3612|year old elementa...|                6|{love -> NULL, op...|   0.96756154|\n",
            "|UgxwacAhzSdfyuqop...|          @jocareers|UC1rhe70J35q1th81...|more informative ...|       1.0|2023-06-23 08:26:51|2023-06-23 08:26:51|            1.0|qtlUwwtvuEg|more informative ...|informative aweso...|   0.6249|informative aweso...|                7|{love -> NULL, op...|    0.9724453|\n",
            "|UgxYGoNCpHvUwiFxw...|@nongnitteerawata...|UCajwhzRIoFYkJWVf...|Thanks for creati...|       1.0|2023-07-16 10:56:53|2023-07-16 10:56:53|            1.0|qtlUwwtvuEg|Thanks for creati...|thanks creating s...|   0.7845|thanks creating s...|                8|{love -> NULL, op...|   0.97572625|\n",
            "|UgxqkhcAmRdtlnchF...|        @1234sanarch|UCHSS7VPbU3txzUU3...|nice narration an...|       1.0|2023-06-19 03:05:39|2023-06-19 03:05:39|            3.0|qtlUwwtvuEg|nice narration an...|nice narration pr...|    0.802|nice narration pr...|                9|{love -> NULL, op...|    0.9866275|\n",
            "+--------------------+--------------------+--------------------+--------------------+----------+-------------------+-------------------+---------------+-----------+--------------------+--------------------+---------+--------------------+-----------------+--------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_with_sentiment.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xaoXhTR4_T6T"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/24 01:04:00 WARN TaskSetManager: Stage 12 contains a task of very large size (9365 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------+-------------+\n",
            "|   video_id|like_count|emotion_score|\n",
            "+-----------+----------+-------------+\n",
            "|qtlUwwtvuEg|       1.0|    0.9666206|\n",
            "|qtlUwwtvuEg|       4.0|   0.98379844|\n",
            "|qtlUwwtvuEg|       2.0|    0.9885464|\n",
            "|qtlUwwtvuEg|      13.0|    0.9950938|\n",
            "|qtlUwwtvuEg|       1.0|   0.98620284|\n",
            "|qtlUwwtvuEg|       2.0|  0.014072355|\n",
            "|qtlUwwtvuEg|       3.0|   0.96756154|\n",
            "|qtlUwwtvuEg|       1.0|    0.9724453|\n",
            "|qtlUwwtvuEg|       1.0|   0.97572625|\n",
            "|qtlUwwtvuEg|       1.0|    0.9866275|\n",
            "|qtlUwwtvuEg|       1.0|   0.97977394|\n",
            "|qtlUwwtvuEg|       1.0|  0.007977011|\n",
            "|qtlUwwtvuEg|       1.0|    0.9743988|\n",
            "|qtlUwwtvuEg|       1.0|    0.9740914|\n",
            "|qtlUwwtvuEg|      18.0|    0.3058869|\n",
            "|qtlUwwtvuEg|       2.0|   0.14363132|\n",
            "|qtlUwwtvuEg|       1.0|    0.9972146|\n",
            "|qtlUwwtvuEg|       1.0|    0.7606054|\n",
            "|qtlUwwtvuEg|       1.0|     0.986654|\n",
            "|qtlUwwtvuEg|       1.0|    0.9924589|\n",
            "|qtlUwwtvuEg|       1.0|    0.7254981|\n",
            "|qtlUwwtvuEg|       9.0|   0.98634845|\n",
            "|qtlUwwtvuEg|       1.0|    0.9671725|\n",
            "|qtlUwwtvuEg|       1.0|   0.58021796|\n",
            "|qtlUwwtvuEg|       1.0|    0.9731096|\n",
            "|qtlUwwtvuEg|       1.0|    0.9839061|\n",
            "|qtlUwwtvuEg|       1.0|    0.9515108|\n",
            "|qtlUwwtvuEg|       1.0|    0.6697492|\n",
            "|qtlUwwtvuEg|       1.0|    0.9732622|\n",
            "|qtlUwwtvuEg|       1.0|    0.9020851|\n",
            "|qtlUwwtvuEg|       1.0|  0.014072355|\n",
            "|qtlUwwtvuEg|       1.0|   0.96510017|\n",
            "|qtlUwwtvuEg|       4.0|    0.9663092|\n",
            "|qtlUwwtvuEg|       1.0|    0.8177947|\n",
            "|qtlUwwtvuEg|       1.0|   0.96897745|\n",
            "|qtlUwwtvuEg|       1.0|   0.98740906|\n",
            "|qtlUwwtvuEg|       1.0|   0.51427853|\n",
            "|qtlUwwtvuEg|       1.0|   0.67327875|\n",
            "|qtlUwwtvuEg|       1.0|     0.920935|\n",
            "|qtlUwwtvuEg|       1.0|     0.976585|\n",
            "|qtlUwwtvuEg|       2.0|   0.01560871|\n",
            "|qtlUwwtvuEg|       2.0|   0.58021796|\n",
            "|qtlUwwtvuEg|       1.0|   0.98769397|\n",
            "|qtlUwwtvuEg|       3.0|    0.1081977|\n",
            "|qtlUwwtvuEg|       2.0|   0.97575736|\n",
            "|qtlUwwtvuEg|       1.0|    0.9877905|\n",
            "|qtlUwwtvuEg|       1.0|    0.9698327|\n",
            "|qtlUwwtvuEg|       1.0|   0.97615224|\n",
            "|qtlUwwtvuEg|       1.0|   0.61577314|\n",
            "|qtlUwwtvuEg|       1.0|    0.9848573|\n",
            "|qtlUwwtvuEg|       1.0|    0.9852118|\n",
            "|qtlUwwtvuEg|       1.0|    0.9768678|\n",
            "|qtlUwwtvuEg|       0.0|   0.97797066|\n",
            "|qtlUwwtvuEg|       1.0|   0.96782404|\n",
            "|qtlUwwtvuEg|       1.0|    0.9673259|\n",
            "|qtlUwwtvuEg|       2.0|    0.9879006|\n",
            "|qtlUwwtvuEg|       1.0|   0.84157705|\n",
            "|qtlUwwtvuEg|       2.0|   0.98730874|\n",
            "|qtlUwwtvuEg|       1.0|   0.98481613|\n",
            "|qtlUwwtvuEg|       1.0|   0.96915996|\n",
            "|qtlUwwtvuEg|       1.0|   0.98839337|\n",
            "|qtlUwwtvuEg|       2.0|   0.97579485|\n",
            "|qtlUwwtvuEg|       1.0|    0.9856159|\n",
            "|qtlUwwtvuEg|       1.0|    0.9864045|\n",
            "|qtlUwwtvuEg|       0.0|    0.9833544|\n",
            "|qtlUwwtvuEg|       0.0|   0.57654715|\n",
            "|qtlUwwtvuEg|       1.0|    0.9773331|\n",
            "|qtlUwwtvuEg|       1.0|    0.9844975|\n",
            "|qtlUwwtvuEg|       1.0|-0.0062487707|\n",
            "|qtlUwwtvuEg|       1.0|   0.29314348|\n",
            "|qtlUwwtvuEg|       1.0|    0.8611977|\n",
            "|qtlUwwtvuEg|       1.0|   0.97041935|\n",
            "|qtlUwwtvuEg|       3.0|     0.971549|\n",
            "|qtlUwwtvuEg|       2.0|    0.9841571|\n",
            "|qtlUwwtvuEg|       1.0|    0.9610977|\n",
            "|qtlUwwtvuEg|       1.0|    0.9849656|\n",
            "|qtlUwwtvuEg|       1.0|    0.9909663|\n",
            "|qtlUwwtvuEg|       1.0|    0.9894055|\n",
            "|qtlUwwtvuEg|       2.0|   0.98928535|\n",
            "|qtlUwwtvuEg|       1.0|   0.98332125|\n",
            "|qtlUwwtvuEg|       1.0|   0.90859306|\n",
            "|qtlUwwtvuEg|       1.0|    0.9317789|\n",
            "|qtlUwwtvuEg|       1.0|   0.99124783|\n",
            "|qtlUwwtvuEg|       1.0|  0.003866174|\n",
            "|qtlUwwtvuEg|       2.0|    0.9818165|\n",
            "|qtlUwwtvuEg|       2.0|   0.96941054|\n",
            "|qtlUwwtvuEg|       1.0|    0.9922733|\n",
            "|qtlUwwtvuEg|       1.0|   0.98824877|\n",
            "|qtlUwwtvuEg|       1.0|   0.97645515|\n",
            "|qtlUwwtvuEg|       1.0|   0.97579485|\n",
            "|qtlUwwtvuEg|       2.0|    0.9863595|\n",
            "|qtlUwwtvuEg|       1.0|   0.97615224|\n",
            "|qtlUwwtvuEg|       1.0|   0.97579485|\n",
            "|qtlUwwtvuEg|       0.0|   0.98708314|\n",
            "|qtlUwwtvuEg|       1.0|   0.98626125|\n",
            "|qtlUwwtvuEg|       4.0|    0.9966133|\n",
            "|qtlUwwtvuEg|       1.0|   0.97647816|\n",
            "|qtlUwwtvuEg|       1.0|   0.97797805|\n",
            "|qtlUwwtvuEg|       1.0|    0.9901475|\n",
            "|qtlUwwtvuEg|       1.0|   0.97417545|\n",
            "+-----------+----------+-------------+\n",
            "only showing top 100 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_final = df_with_sentiment.select(\"video_id\", \"like_count\", \"emotion_score\")\n",
        "df_final.show(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so1bXnaU_T6U"
      },
      "source": [
        "# PySpark: SQL query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GryEetMY_T6U"
      },
      "outputs": [],
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "df_final.createOrReplaceTempView(\"sentiment_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0BgJqMlG_T6U"
      },
      "outputs": [],
      "source": [
        "# Execute SQL query to apply the same logic as the PySpark code\n",
        "df_weighted_sentiment_sum = spark.sql(\"\"\"\n",
        "    WITH subquery AS (\n",
        "    SELECT\n",
        "        video_id,\n",
        "        SUM(like_count + 1) AS total_likes,\n",
        "        SUM((like_count + 1) * emotion_score) AS score_x_emotion\n",
        "    FROM\n",
        "        sentiment_data\n",
        "    GROUP BY\n",
        "        video_id\n",
        "    )\n",
        "    SELECT\n",
        "        s.video_id,\n",
        "        (s.score_x_emotion / s.total_likes) AS final_emotion_score\n",
        "    FROM subquery s\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m3Oko-bc_T6V"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/24 01:07:19 WARN TaskSetManager: Stage 25 contains a task of very large size (9365 KiB). The maximum recommended task size is 1000 KiB.\n",
            "[Stage 25:>                                                       (0 + 12) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------------+\n",
            "|   video_id|final_emotion_score|\n",
            "+-----------+-------------------+\n",
            "|96-u9s6D16k| 0.5903172061849506|\n",
            "|s1qZbsytk08|0.07676181044234594|\n",
            "|erFxR8DNu_Q| 0.9868854880332947|\n",
            "|M9a0XrkdBf4| 0.7205628536175936|\n",
            "|wQ8BIBpya2k|0.27070582827040085|\n",
            "|LeJkB_Moejs| 0.5637741315877065|\n",
            "|vq2nnJ4g6N0| 0.2807055507151728|\n",
            "|fxYtAtIq8rY| 0.3508107556651036|\n",
            "|957fQCm5aDo|0.34214369648173687|\n",
            "|BJ6kyj-st9k|0.29239623910182394|\n",
            "+-----------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_weighted_sentiment_sum.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/24 01:16:45 WARN TaskSetManager: Stage 34 contains a task of very large size (9365 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1767"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_weighted_sentiment_sum.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/24 01:15:15 WARN TaskSetManager: Stage 31 contains a task of very large size (9365 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Save data as parts \n",
        "df_weighted_sentiment_sum.write.csv(\"../data/clean/df_videoid_emotionscore.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqXcBfKmSmMk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhUrjLUU_T6V"
      },
      "source": [
        "# Plotting test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AeYcPRr_T6V"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample data for a bubble heatmap\n",
        "data = np.random.choice([0, 1, 2], size=(10, 10))  # Random data with values 0, 1, 2\n",
        "df = pd.DataFrame(data, columns=[f\"Column {i+1}\" for i in range(10)],\n",
        "                  index=[f\"Row {i+1}\" for i in range(10)])\n",
        "\n",
        "# Create the bubble heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        value = df.iloc[i, j]\n",
        "        if value != 0:\n",
        "            plt.scatter(j, i, s=value * 300, color='blue' if value == 1 else 'purple', alpha=0.6)\n",
        "\n",
        "# Customizing the heatmap\n",
        "plt.xticks(np.arange(df.shape[1]), df.columns, rotation=90)\n",
        "plt.yticks(np.arange(df.shape[0]), df.index)\n",
        "plt.gca().invert_yaxis()  # To match the orientation\n",
        "\n",
        "plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Creating a custom legend\n",
        "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label='Probably', markersize=10,\n",
        "                               markerfacecolor='blue', alpha=0.6),\n",
        "                   plt.Line2D([0], [0], marker='o', color='w', label='Could be', markersize=10,\n",
        "                               markerfacecolor='purple', alpha=0.6)]\n",
        "\n",
        "plt.legend(handles=legend_elements, title='Key', loc='upper right')\n",
        "\n",
        "plt.title('Bubble Heatmap Example', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdO1vi6y_T6W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"Data science is an interdisciplinary field.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is widely used in data science and machine learning.\",\n",
        "    \"TF-IDF is a statistical measure used in text mining.\",\n",
        "    \"Visualization is key in presenting data analysis.\"\n",
        "]\n",
        "\n",
        "# Generate TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Create a DataFrame from the TF-IDF matrix\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Define a threshold for low values\n",
        "low_value_threshold = 0.1\n",
        "\n",
        "# Creating a bubble map\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Iterate over the DataFrame to create the bubble map with crosses for low values\n",
        "for row_idx in range(tfidf_df.shape[0]):\n",
        "    for col_idx in range(tfidf_df.shape[1]):\n",
        "        value = tfidf_df.iloc[row_idx, col_idx]\n",
        "        if value > 0:\n",
        "            # Plot bubbles for non-zero values\n",
        "            plt.scatter(col_idx, row_idx, s=value * 1000, alpha=0.6, color='blue')\n",
        "            # Plot a cross for low TF-IDF values\n",
        "            if value < low_value_threshold:\n",
        "                plt.scatter(col_idx, row_idx, s=100, marker='x', color='red', alpha=0.8, linewidths=2)\n",
        "\n",
        "# Customizing the bubble map\n",
        "plt.xticks(np.arange(tfidf_df.shape[1]), tfidf_df.columns, rotation=90)\n",
        "plt.yticks(np.arange(tfidf_df.shape[0]), [f\"Doc {i+1}\" for i in range(tfidf_df.shape[0])])\n",
        "plt.gca().invert_yaxis()  # To match the heatmap orientation\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "\n",
        "plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Title and layout adjustments\n",
        "plt.title('Bubble Map of TF-IDF Matrix with Crosses for Low Values', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhQybc6B_T6W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Crear los datos de ejemplo similares al gráfico que compartiste\n",
        "emotions = [\n",
        "    \"Abysmal\", \"Awful\", \"Terrible\", \"Very bad\", \"Really bad\", \"Rubbish\", \"Bad\", \"Poor\",\n",
        "    \"Pretty bad\", \"Below average\", \"Mediocre\", \"Average\", \"Not bad\", \"Above average\",\n",
        "    \"Pretty good\", \"Good\", \"Great\", \"Really good\", \"Very good\", \"Fantastic\", \"Superb\",\n",
        "    \"Brilliant\", \"Incredible\"\n",
        "]\n",
        "average_scores = np.linspace(1.2, 8.8, len(emotions))\n",
        "\n",
        "# Creamos una lista de distribuciones ficticias para cada emoción\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'score': np.concatenate([np.random.normal(loc=avg, scale=0.4, size=500) for avg in average_scores]),\n",
        "    'emotion': np.concatenate([[emotion] * 500 for emotion in emotions])\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['score'] = df['score'].clip(0, 10)  # Limitar valores de score entre 0 y 10\n",
        "\n",
        "# Crear el gráfico estilo joyplot manualmente asegurando el desplazamiento vertical y la alineación con la cuadrícula\n",
        "plt.figure(figsize=(10, 14))\n",
        "colors = plt.cm.RdYlGn(np.linspace(0, 1, len(emotions)))\n",
        "\n",
        "# Configuración del desplazamiento para permitir superposición y alineación\n",
        "offset = 0\n",
        "overlap_factor = 0.5  # Ajusta este factor para la superposición\n",
        "\n",
        "# Generar los datos de densidad para cada emoción\n",
        "for i, emotion in enumerate(reversed(emotions)):\n",
        "    subset = df[df['emotion'] == emotion]\n",
        "    density, bins = np.histogram(subset['score'], bins=150, range=(0, 10), density=True)\n",
        "    bins_center = 0.5 * (bins[1:] + bins[:-1])\n",
        "\n",
        "    # Dibujar el área sombreada con color y permitir superposición\n",
        "    plt.fill_between(bins_center, offset, density + offset, color=colors[i], alpha=0.2, step='mid')\n",
        "\n",
        "    # Dibujar la línea de la curva de densidad sobre el área coloreada\n",
        "    plt.plot(bins_center, density + offset, color='black', linewidth=0.8)\n",
        "\n",
        "    # Incrementar el offset para la siguiente línea permitiendo superposición y alineación con la cuadrícula\n",
        "    offset += max(density) * overlap_factor\n",
        "\n",
        "# Configuraciones del gráfico\n",
        "plt.yticks(np.arange(0, offset, offset / len(emotions)), reversed(emotions))  # Asegurar alineación\n",
        "plt.xticks(np.arange(0, 11, 1))  # Alinear los ticks del eje x con la cuadrícula\n",
        "plt.xlabel(\"Emotional Intensity (0 = Very Negative, 10 = Very Positive)\")\n",
        "plt.ylabel(\"Emotion\")\n",
        "plt.title(\"Emotional Analysis of YouTube Video Comments\", fontsize=14)\n",
        "plt.xlim(0, 10)\n",
        "\n",
        "# Mejorar la visualización de la cuadrícula\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.grid(axis='y', linestyle=':', alpha=0.2)\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FhUrjLUU_T6V"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
