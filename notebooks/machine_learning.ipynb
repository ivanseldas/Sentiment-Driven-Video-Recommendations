{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../data/clean_data/'\n",
    "\n",
    "df_categories = pd.read_csv(f'{folder}df_categories.csv')\n",
    "df_comments = pd.read_parquet(f'{folder}df_comments_video.parquet')\n",
    "df_transcript = pd.read_csv(f'{folder}df_transcript_original.csv')\n",
    "df_videos = pd.read_csv(f'{folder}df_video_data.csv')\n",
    "df_channel = pd.read_csv(f'{folder}df_channel_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizaci칩n Textos TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 1\n",
    "\n",
    "Variables: `clean_text`, `video_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>music thank hello everyone hope great era ai w...</td>\n",
       "      <td>qtlUwwtvuEg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>number three music facebook enacted emergency ...</td>\n",
       "      <td>QaoDXYYtgK0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>series going introduce deep learning least per...</td>\n",
       "      <td>PqDwddEHswU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn money ai part two let go want create kind...</td>\n",
       "      <td>B-Y7rnOa43w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>current medical science run option doctor dont...</td>\n",
       "      <td>vyit-1zKsZ4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>good afternoon started recording right welcome...</td>\n",
       "      <td>uuh7spVdf0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>machine learning big data changing econometric...</td>\n",
       "      <td>Bm6CAjVtrIw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>music applause thanks much hadley thank coming...</td>\n",
       "      <td>atiYXm7JZv0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>happened last year made urgent long time worki...</td>\n",
       "      <td>ZQazWxegNm8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>there saying go great power come great respons...</td>\n",
       "      <td>CkG15bX4z90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1496 rows 칑 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text     video_id\n",
       "0     music thank hello everyone hope great era ai w...  qtlUwwtvuEg\n",
       "1     number three music facebook enacted emergency ...  QaoDXYYtgK0\n",
       "2     series going introduce deep learning least per...  PqDwddEHswU\n",
       "3     earn money ai part two let go want create kind...  B-Y7rnOa43w\n",
       "4     current medical science run option doctor dont...  vyit-1zKsZ4\n",
       "...                                                 ...          ...\n",
       "1781  good afternoon started recording right welcome...  uuh7spVdf0c\n",
       "1782  machine learning big data changing econometric...  Bm6CAjVtrIw\n",
       "1783  music applause thanks much hadley thank coming...  atiYXm7JZv0\n",
       "1785  happened last year made urgent long time worki...  ZQazWxegNm8\n",
       "1786  there saying go great power come great respons...  CkG15bX4z90\n",
       "\n",
       "[1496 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = df_transcript[df_transcript['language'] == 'english'][['clean_text', 'video_id']].copy()\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El par치metro ngram_range en TfidfVectorizer se refiere a los tama침os de n-gramas que el vectorizador debe considerar al analizar el texto. Un n-grama es una secuencia de \"n\" palabras consecutivas en un texto.\n",
    "\n",
    "Desglose de ngram_range=(1, 2):\n",
    "Unigrama (1-gram): Es una secuencia de 1 palabra. Si ngram_range=(1, 1), el vectorizador solo considerar칤a palabras individuales.\n",
    "\n",
    "Ejemplo: Para la frase \"hola mundo\", los unigramas ser칤an: [\"hola\", \"mundo\"].\n",
    "Bigramas (2-gram): Es una secuencia de 2 palabras consecutivas. Si ngram_range=(2, 2), el vectorizador solo considerar칤a pares de palabras consecutivas.\n",
    "\n",
    "Ejemplo: Para la frase \"hola mundo\", los bigramas ser칤an: [\"hola mundo\"].\n",
    "ngram_range=(1, 2): Con este rango, el vectorizador considerar치 tanto unigramas como bigramas. Es decir, se generar치n caracter칤sticas tanto para palabras individuales como para pares de palabras consecutivas.\n",
    "\n",
    "Ejemplo: Para la frase \"hola mundo\", los n-gramas ser칤an: [\"hola\", \"mundo\", \"hola mundo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_transcript: (1496, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crear el vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Aplicar TF-IDF a comentarios\n",
    "# tfidf_comments = tfidf_vectorizer.fit_transform(model1['clean_text'])\n",
    "tfidf_transcripts = tfidf_vectorizer.fit_transform(model1['clean_text'])\n",
    "\n",
    "# Convertir a DataFrame para visualizar las caracter칤sticas\n",
    "# tfidf_comments_df = pd.DataFrame(tfidf_comments.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_transcripts_df = pd.DataFrame(tfidf_transcripts.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# print(f'tfidf_comments: {tfidf_comments_df.shape}')\n",
    "print(f'tfidf_transcript: {tfidf_transcripts_df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get recommendations\n",
    "\n",
    "La similitud coseno (cosine similarity) es una medida de similitud entre dos vectores en un espacio vectorial que calcula el coseno del 치ngulo entre ellos. Es ampliamente utilizada en procesamiento de lenguaje natural (NLP), recuperaci칩n de informaci칩n, y otras 치reas relacionadas con la comparaci칩n de texto o documentos, ya que es eficiente y proporciona una buena estimaci칩n de la similitud entre documentos o textos representados como vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98644005, 0.96832966, 0.96590921, 0.99591   ])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity([[1, 1, 3], [1, 2, 4]], [[1, 1, 5], [1, 3, 5]])\n",
    "similarity.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video watched: AI in Transportation From Autonomous Vehicles to Traffic Management #shorts #ai [0CqyPmLw8X4]\n",
      "\n",
      "videos recommended:       video_id  similarity                                              title\n",
      "0  v6OB80Vt1Dk    1.000000     5 Mind-blowing Artificial Intelligence Tools 游뱚\n",
      "1  P_TAklE-UmE    0.360151  Kittl AI Unveiled: The Unstoppable AI Vector G...\n",
      "2  Cu-UBInM2eo    0.330453  Create Stunning Animated Explainer Videos in M...\n",
      "3  EL2woThmPWM    0.310712             $0,00 AI Tools That Can Make You Money\n",
      "4  4p00K5_N7dI    0.298531  游댠 Top 10 Mind Blowing AI Tools For 2023 | 10 B...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suponiendo que conoces el video_id del video para el cual quieres recomendaciones\n",
    "video_id = model1['video_id'][50]  # Reemplaza con el video_id del video espec칤fico\n",
    "\n",
    "# Extraer el 칤ndice del video en el DataFrame\n",
    "index = model1[model1['video_id'] == video_id].index[0]\n",
    "\n",
    "# Calcular la similitud del coseno entre este video y todos los dem치s\n",
    "cosine_similarities = cosine_similarity(tfidf_transcripts[index], tfidf_transcripts).flatten()\n",
    "\n",
    "# Crear un DataFrame con los video_ids y las similitudes correspondientes\n",
    "similar_videos = pd.DataFrame({\n",
    "    'video_id': model1['video_id'],\n",
    "    'similarity': cosine_similarities\n",
    "})\n",
    "\n",
    "# Ordenar los videos por similitud, excluyendo el propio video\n",
    "similar_videos = similar_videos[similar_videos['video_id'] != video_id].sort_values(by='similarity', ascending=False)\n",
    "\n",
    "similar_videos = similar_videos.merge(df_videos[['videoId', 'title']], left_on='video_id', right_on='videoId')\n",
    "similar_videos.drop(columns='videoId', inplace=True)\n",
    "\n",
    "# Mostrar los 5 videos m치s similares\n",
    "video_title = df_videos[df_videos['videoId'] == video_id]['title'].iloc[0]\n",
    "print(f'video watched: {video_title} [{video_id}]\\n')\n",
    "print(f'videos recommended: {similar_videos.head()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
